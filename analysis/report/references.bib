@article{bergerDirectEffectValidity2003,
  title = {Direct Effect on Validity of Response Run-in Selection in Clinical Trials},
  author = {Berger, Vance W and Rezvani, Azadeh and Makarewicz, Vanessa A},
  year = {2003},
  month = apr,
  journal = {Controlled Clinical Trials},
  volume = {24},
  number = {2},
  pages = {156--166},
  issn = {0197-2456},
  doi = {10.1016/S0197-2456(02)00316-1},
  urldate = {2022-10-20},
  abstract = {A run-in is a period prior to randomization during which potential participants who have met all entry criteria for a randomized clinical trial are assigned the same regimen, either the control (possibly placebo) or the experimental treatment. Typically, the intention is to exclude from the subsequent study (i.e., randomization) some segment of this cohort, based on their experiences during the run-in period. Selecting patients based on the run-in thereby forces differential representation of certain subpopulations relative to others. Previous studies have addressed the potential for a run-in to jeopardize validity through unintended mechanisms. While these concerns are valid, they leave open the possibility that modifications to the design of the run-in might be able to preserve validity, even if patient selection is based on the results of the run-in. As such, we address the potential for selecting patients based on response during a run-in period to jeopardize validity directly, through its intended effect of overrepresenting (relative to the screened population) some segments in the randomized portion of the trial and underrepresenting others. Sackett D, Vist G. {$<$}},
  langid = {english},
  keywords = {Clinical relevance,Inference discrepancy,Inference population,Target population},
  file = {/Users/zenn/Zotero/storage/FUCT7KZ4/Berger et al. - 2003 - Direct effect on validity of response run-in selec.pdf;/Users/zenn/Zotero/storage/9HRAMYTI/S0197245602003161.html}
}

@article{cobigoDetectionEmergingNeurodegeneration2022,
  title = {Detection of Emerging Neurodegeneration Using {{Bayesian}} Linear Mixed-Effect Modeling},
  author = {Cobigo, Yann and Goh, Matthew S. and Wolf, Amy and Staffaroni, Adam M. and Kornak, John and Miller, Bruce L. and Rabinovici, Gil D. and Seeley, William W. and Spina, Salvatore and Boxer, Adam L. and Boeve, Bradley F. and Wang, Lei and Allegri, Ricardo and Farlow, Marty and Mori, Hiroshi and Perrin, Richard J. and Kramer, Joel and Rosen, Howard J.},
  year = {2022},
  month = jan,
  journal = {NeuroImage: Clinical},
  volume = {36},
  pages = {103144},
  issn = {2213-1582},
  doi = {10.1016/j.nicl.2022.103144},
  urldate = {2022-10-20},
  abstract = {Early detection of neurodegeneration, and prediction of when neurodegenerative diseases will lead to symptoms, are critical for developing and initiating disease modifying treatments for these disorders. While each neurodegenerative disease has a typical pattern of early changes in the brain, these disorders are heterogeneous, and early manifestations can vary greatly across people. Methods for detecting emerging neurodegeneration in any part of the brain are therefore needed. Prior publications have described the use of Bayesian linear mixed-effects (BLME) modeling for characterizing the trajectory of change across the brain in healthy controls and patients with neurodegenerative disease. Here, we use an extension of such a model to detect emerging neurodegeneration in cognitively healthy individuals at risk for dementia. We use BLME to quantify individualized rates of volume loss across the cerebral cortex from the first two MRIs in each person and then extend the BLME model to predict future values for each voxel. We then compare observed values at subsequent time points with the values that were expected from the initial rates of change and identify voxels that are lower than the expected values, indicating accelerated volume loss and neurodegeneration. We apply the model to longitudinal imaging data from cognitively normal participants in the Alzheimer's Disease Neuroimaging Initiative (ADNI), some of whom subsequently developed dementia, and two cognitively normal cases who developed pathology-proven frontotemporal lobar degeneration (FTLD). These analyses identified regions of accelerated volume loss prior to or accompanying the earliest symptoms, and expanding across the brain over time, in all cases. The changes were detected in regions that are typical for the likely diseases affecting each patient, including medial temporal regions in patients at risk for Alzheimer's disease, and insular, frontal, and/or anterior/inferior temporal regions in patients with likely or proven FTLD. In the cases where detailed histories were available, the first regions identified were consistent with early symptoms. Furthermore, survival analysis in the ADNI cases demonstrated that the rate of spread of accelerated volume loss across the brain was a statistically significant predictor of time to conversion to dementia. This method for detection of neurodegeneration is a potentially promising approach for identifying early changes due to a variety of diseases, without prior assumptions about what regions are most likely to be affected first in an individual.},
  langid = {english},
  keywords = {Alzheimer’s Disease,Bayesian linear mixed-effect,Bayesian prediction,Frontotemporal Lobar Degeneration},
  file = {/Users/zenn/Zotero/storage/K5VK3DI8/Cobigo et al. - 2022 - Detection of emerging neurodegeneration using Baye.pdf;/Users/zenn/Zotero/storage/TJRVIV8K/S2213158222002091.html}
}

@article{frisonLinearlyDivergentTreatment1997,
  title = {Linearly Divergent Treatment Effects in Clinical Trials with Repeated Measures: Efficient Analysis Using Summary Statistics},
  shorttitle = {Linearly Divergent Treatment Effects in Clinical Trials with Repeated Measures},
  author = {Frison, Lars J. and Pocock, Stuart J.},
  year = {1997},
  month = dec,
  journal = {Statistics in Medicine},
  volume = {16},
  number = {24},
  pages = {2855--2872},
  issn = {0277-6715, 1097-0258},
  doi = {10.1002/(SICI)1097-0258(19971230)16:24<2855::AID-SIM749>3.0.CO;2-Y},
  urldate = {2023-04-14},
  abstract = {In many randomized clinical trials with repeated measures of a response variable one anticipates a linear divergence over time in the difference between treatments. This paper explores how to make an efficient choice of analysis based on individual patient summary statistics. With the objective of estimating the mean rate of treatment divergence the simplest choice of summary statistic is the regression coefficient of response on time for each subject (SLOPE). The gains in statistical efficiency imposed by adjusting for the observed pre-treatment levels, or even better the estimated intercepts, are clarified. In the process, we develop the optimal linear summary statistic for any repeated measures design with assumed known covariance structure and shape of true mean treatment difference over time. Statistical power considerations are explored and an example from an asthma trial is used to illustrate the main points. 1997 by John Wiley \& Sons, Ltd.},
  langid = {english},
  file = {/Users/zenn/Zotero/storage/NVEYW46M/Frison and Pocock - 1997 - Linearly divergent treatment effects in clinical t.pdf}
}

@article{frostAnalysisRepeatedDirect2004,
  title = {The Analysis of Repeated `Direct' Measures of Change Illustrated with an Application in Longitudinal Imaging},
  author = {Frost, Chris and Kenward, Michael G. and Fox, Nick C.},
  year = {2004},
  month = nov,
  journal = {Statistics in Medicine},
  volume = {23},
  number = {21},
  pages = {3275--3286},
  issn = {02776715, 10970258},
  doi = {10.1002/sim.1909},
  urldate = {2022-12-05},
  abstract = {The use of repeated measures of an outcome variable to improve statistical power and precision in randomized clinical trials and cohort studies is well documented. Linear mixed models have great utility in the analysis of such studies in many medical applications including imaging. However, in imaging studies and other applications the basic outcome can be a `direct' measure of change in a variable, as opposed to a di erence calculated by subtraction of one measured value from another. The correlation structure of such repeated measures of `direct' change, in particular the non-independence of within-person consecutive measures, adds complexity to the analysis. In this paper, we present a family of hierarchical mixed models for the analysis of such data and explain how to implement them using standard statistical software. We illustrate the use of our models with data from a cohort of patients with Alzheimer's disease. Copyright ? 2004 John Wiley \& Sons, Ltd.},
  langid = {english},
  file = {/Users/zenn/Zotero/storage/BC8NFIAB/Frost et al. - 2004 - The analysis of repeated ‘direct’ measures of chan.pdf}
}

@article{frostOptimizingDesignClinical2008,
  title = {Optimizing the Design of Clinical Trials Where the Outcome Is a Rate. {{Can}} Estimating a Baseline Rate in a Run-in Period Increase Efficiency?: {{THE EFFICIENCY OF CLINICAL TRIALS WITH A RUN-IN DESIGN}}},
  shorttitle = {Optimizing the Design of Clinical Trials Where the Outcome Is a Rate. {{Can}} Estimating a Baseline Rate in a Run-in Period Increase Efficiency?},
  author = {Frost, Chris and Kenward, Michael G. and Fox, Nick C.},
  year = {2008},
  month = aug,
  journal = {Statistics in Medicine},
  volume = {27},
  number = {19},
  pages = {3717--3731},
  issn = {02776715},
  doi = {10.1002/sim.3280},
  urldate = {2023-04-14},
  abstract = {It is well known that the statistical power of randomized controlled trials with a continuous outcome can be increased by using a pre-randomization baseline measure of the outcome variable as a covariate in the analysis. For a trial where the outcome measure is a rate, for example in a therapeutic trial in Alzheimer's disease, the relevant covariate is a pre-randomization measure of that rate. Obtaining this requires separating the total follow-up period into two periods. In the first `run-in' period all patients would be `off-treatment' to facilitate the calculation of baseline atrophy rates. In the second `on-treatment' period half of the patients, selected at random, would be switched onto active treatment with the others remaining off treatment. In this paper we use linear mixed models to establish a methodological framework that is then used to assess the extent to which such designs can increase statistical power. We illustrate our methodology with two examples. The first is a design with three evenly spaced time points analysed with a standard random slopes model. The second is a model for repeated `direct' measures of changes used for the analysis of imaging studies with visits at multiple time points. We show that run-in designs can materially reduce sample size provided that true between-subject variability in rates is large relative to measurement error. Copyright q 2008 John Wiley \& Sons, Ltd.},
  langid = {english},
  file = {/Users/zenn/Zotero/storage/KVI48I9K/Frost et al. - 2008 - Optimizing the design of clinical trials where the.pdf}
}

@article{huPowerSampleSize2021,
  title = {Power and Sample Size for Random Coefficient Regression Models in Randomized Experiments with Monotone Missing Data},
  author = {Hu, Nan and Mackey, Howard and Thomas, Ronald},
  year = {2021},
  month = apr,
  journal = {Biometrical Journal},
  volume = {63},
  number = {4},
  pages = {806--824},
  issn = {0323-3847, 1521-4036},
  doi = {10.1002/bimj.202000184},
  urldate = {2022-12-05},
  abstract = {Random coefficient regression (also known as random effects, mixed effects, growth curve, variance component, multilevel, or hierarchical linear modeling) can be a natural and useful approach for characterizing and testing hypotheses in data that are correlated within experimental units. Existing power and sample size software for such data are based on two variance component models or those using a two-stage formulation. These approaches may be markedly inaccurate in settings where more variance components (i.e., intercept, rate of change, and residual error) are warranted. We present variance, power, sample size formulae, and software (R Shiny app) for use with random coefficient regression models with possible missing data and variable follow-up. We illustrate sample size and study design planning using data from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database. We additionally examine the drivers of variability to better inform study design.},
  langid = {english},
  file = {/Users/zenn/Zotero/storage/WP6Z5542/Hu et al. - 2021 - Power and sample size for random coefficient regre.pdf}
}

@article{kennedyUsingBaselineCognitive2015,
  title = {Using Baseline Cognitive Severity for Enriching {{Alzheimer}}'s Disease Clinical Trials: {{How}} Does {{Mini-Mental State Examination}} Predict Rate of Change?},
  shorttitle = {Using Baseline Cognitive Severity for Enriching {{Alzheimer}}'s Disease Clinical Trials},
  author = {Kennedy, Richard E. and Cutter, Gary R. and Wang, Guoqiao and Schneider, Lon S.},
  year = {2015},
  month = jun,
  journal = {Alzheimer's \& Dementia: Translational Research \& Clinical Interventions},
  volume = {1},
  number = {1},
  pages = {46--52},
  issn = {2352-8737},
  doi = {10.1016/j.trci.2015.03.001},
  urldate = {2022-10-20},
  abstract = {Background Post hoc analyses from clinical trials in Alzheimer's disease (AD) suggest that more cognitively impaired participants respond differently from less impaired on cognitive outcomes. We examined pooled clinical trials data to assess the utility of enriching trials using baseline cognition. Methods We included 2882 participants with mild to moderate AD in seven studies from a meta-database. We used mixed effects models to estimate the rate of decline in Alzheimer's disease Assessment Scale-cognitive (ADAS-Cog) scores among Mini-Mental State Examination (MMSE) groups. Findings Baseline MMSE category was associated with baseline scores and rate of decline on the ADAS-Cog, adjusting for age and education (both P~{$<~$}.001). Greater baseline cognitive impairment was associated with more rapid progression. Interpretations Although we found significant differences in rate of decline, most differences between individuals were from baseline ADAS-Cog values. Since enrichment based on MMSE would reduce the recruitment pool while adding only slightly to detecting differences in rate of progression, it is not advised.},
  langid = {english},
  keywords = {Alzheimer disease,Alzheimer's Disease Assessment Scale,Alzheimer's Disease Cooperative study (ADCS),Alzheimer's Disease Neuroimaging Initiative (ADNI),Clinical trials,Clinical trials and methods,Mini-Mental State Examination,Simulations},
  file = {/Users/zenn/Zotero/storage/YU92IWZR/Kennedy et al. - 2015 - Using baseline cognitive severity for enriching Al.pdf;/Users/zenn/Zotero/storage/RXNK9GZR/S2352873715000037.html}
}

@article{lairdEstimatingRatesChange1990,
  title = {Estimating Rates of Change in Randomized Clinical Trials},
  author = {Laird, Nan M. and {Fong Wang}},
  year = {1990},
  month = dec,
  journal = {Controlled Clinical Trials},
  volume = {11},
  number = {6},
  pages = {405--419},
  issn = {01972456},
  doi = {10.1016/0197-2456(90)90018-W},
  urldate = {2023-04-14},
  abstract = {This article deals with the extension of the pretest-posttest clinical trial to the longitudinal data setting. We assume that a baseline (or pretest) measurement is taken on all individuals, who are then randomized, without regard to baseline values, to a treatment group. Repeated measurements are taken postrandomization at specified times. Our objective is to estimate the average rate of change (or slope) in the experimental groups and the differences in the slopes. Our focus is on the optimal use of the baseline measurements in the analysis. We contrast two different approaches:-a multivariate one that regards the entire vector of responses (including the baseline) as random outcomes and a univariate one that uses each individual's least squares slope as an outcome. Our multivariate approach is essentially a generalization of Stanek's Seemingly Unrelated Regression (SUR) estimator for the pretest-posttest design (Am Stat 42:178-183, 1988). The multivariate approach is natural to apply in this setting, and optimal if the assumed model is correct. However, the most efficient estimator requires assuming that the baseline mean parameters are the same for all experimental groups. Although this assumption is reasonable in the randomized setting, the resulting multivariate estimator uses postrandomization data as a covariate; if the assumed linear model is not correct, this can lead to distortions in the estimated treatment effect. We propose instead a reduced form multivariate estimator that may be somewhat less efficient, but protects against model misspecification.},
  langid = {english},
  file = {/Users/zenn/Zotero/storage/79SV267C/Laird and Fong Wang - 1990 - Estimating rates of change in randomized clinical .pdf}
}

@article{lairdlRandomEffectsModelsLongitudinal2023,
  title = {Random-{{Effects Models}} for {{Longitudinal Data}}},
  author = {Lairdl, Nan M and Warel, James H},
  year = {2023},
  abstract = {Models for the analysis of longitudinal data must recogrlize the relationship between serial observations on the same unit. Multivariate models with general covariance structure are often difficult to apply to highly unbalanced data, whereas two-stage random-effects models can be used easily. In two-stage models, the probability distributions for the response vectors of different individuals belong to a single family, but some random-effects parameters vary across individuals, with a distribution specified at the second stage. A general family of models is discussed, which includes both growth models and repeated-measures models as special cases. A unified approach to fitting these models, based on a combination of empirical Bayes and maximum likelihood estimation of model parameters and using the EM algorithm, is discussed. Two examples are taken from a current epidemiological study of the health effects of air pollution.},
  langid = {english},
  file = {/Users/zenn/Zotero/storage/NI84LR4S/Lairdl and Warel - 2023 - Random-Effects Models for Longitudinal Data.pdf}
}

@article{luSampleSizeDetermination2009,
  title = {Sample Size Determination for Constrained Longitudinal Data Analysis},
  author = {Lu, Kaifeng and Mehrotra, Devan V. and Liu, Guanghan},
  year = {2009},
  month = feb,
  journal = {Statistics in Medicine},
  volume = {28},
  number = {4},
  pages = {679--699},
  issn = {02776715, 10970258},
  doi = {10.1002/sim.3507},
  urldate = {2022-12-05},
  abstract = {The longitudinal data analysis model proposed by Liang and Zeger (Sankhya\textasciimacron : Indian J. Stat. Ser. B 2000; 62:134\textendash 148) uses the baseline as well as postbaseline values as the dependent variables, and the baseline mean responses are constrained to be the same across treatment groups due to randomization. Compared with the conventional longitudinal analysis of covariance, this approach can correctly estimate the variance of within-group mean changes and achieve the specified coverage probabilities. General results on the sample size and power calculations for this model in the presence of missing data are obtained. The sample size relationship between the constrained and unconstrained longitudinal data analysis is established. Simple expressions for sample size calculation are obtained for the compound symmetry and first-order autoregressive correlation structures. The sensitivity of the sample size requirement to the configuration of correlation structure and retention pattern is assessed. The performance of several ad hoc approximations for longitudinal data analysis sample size calculation is evaluated. Simulation studies are conducted to assess the validity of the proposed sample size formulas with deviation from normality. The sample size formulas are also illustrated in detail using real clinical trial data. Copyright q 2008 John Wiley \& Sons, Ltd.},
  langid = {english},
  file = {/Users/zenn/Zotero/storage/ENTISM78/Lu et al. - 2009 - Sample size determination for constrained longitud.pdf}
}

@article{nashPowerSamplesizeCalculations2021,
  title = {Power and Sample-Size Calculations for Trials That Compare Slopes over Time: {{Introducing}} the Slopepower Command},
  shorttitle = {Power and Sample-Size Calculations for Trials That Compare Slopes over Time},
  author = {Nash, Stephen and Morgan, Katy E. and Frost, Chris and Mulick, Amy},
  year = {2021},
  month = sep,
  journal = {The Stata Journal},
  volume = {21},
  number = {3},
  pages = {575--601},
  publisher = {{SAGE Publications}},
  issn = {1536-867X},
  doi = {10.1177/1536867X211045512},
  urldate = {2022-10-20},
  abstract = {Trials of interventions that aim to slow disease progression may analyze a continuous outcome by comparing its change over time?its slope?between the treated and the untreated group using a linear mixed model. To perform a sample-size calculation for such a trial, one must have estimates of the parameters that govern the between- and within-subject variability in the outcome, which are often unknown. The algebra needed for the sample-size calculation can also be complex for such trial designs. We have written a new user-friendly command, slopepower, that performs sample-size or power calculations for trials that compare slope outcomes. The package is based on linear mixed-model methodology, described for this setting by Frost, Kenward, and Fox (2008, Statistics in Medicine 27: 3717?3731). In the first stage of this approach, slopepower obtains estimates of mean slopes together with variances and covariances from a linear mixed model fit to previously collected user-supplied data. In the second stage, these estimates are combined with user input about the target effectiveness of the treatment and design of the future trial to give an estimate of either a sample size or a statistical power. In this article, we present the slopepower command, briefly explain the methodology behind it, and demonstrate how it can be used to help plan a trial and compare the sample sizes needed for different trial designs.},
  langid = {english},
  file = {/Users/zenn/Zotero/storage/G6XLALQH/Nash et al. - 2021 - Power and sample-size calculations for trials that.pdf}
}

@article{packerWhyHasRunIn2017,
  title = {Why {{Has}} a {{Run-In Period Been}} a {{Design Element}} in {{Most Landmark Clinical Trials}}? {{Analysis}} of the {{Critical Role}} of {{Run-In Periods}} in {{Drug Development}}},
  shorttitle = {Why {{Has}} a {{Run-In Period Been}} a {{Design Element}} in {{Most Landmark Clinical Trials}}?},
  author = {Packer, Milton},
  year = {2017},
  month = sep,
  journal = {Journal of Cardiac Failure},
  volume = {23},
  number = {9},
  pages = {697--699},
  issn = {1071-9164},
  doi = {10.1016/j.cardfail.2017.07.401},
  urldate = {2022-10-20},
  abstract = {Prior exposure to one of the randomized treatments has been a routine design element of large-scale trials in patients at high cardiovascular risk. A run-in feature has allowed our trials to be more realistic; it has strengthened their ability to estimate the true treatment effect; and it has never undermined the validity of a trial's findings. Those who suggest that run-in periods distort the results of large-scale trials should become more familiar with our history of drug development and our standards of clinical practice. Physicians use run-in periods every day in real life, and trialists have used run-in periods for decades to reliably establish the role of new cardiovascular drugs. Those who reflexively criticize the trials because of their inclusion of a run-in period need to carefully reexamine how medicine is practiced and how it advances.},
  langid = {english},
  keywords = {Clinical trial design,drug efficacy studies,heart failure trials,run-in periods},
  file = {/Users/zenn/Zotero/storage/LQSL2JD6/Packer - 2017 - Why Has a Run-In Period Been a Design Element in M.pdf;/Users/zenn/Zotero/storage/BMBK8JYV/S1071916417306206.html}
}

@article{schottReducedSampleSizes2010,
  title = {Reduced Sample Sizes for Atrophy Outcomes in {{Alzheimer}}'s Disease Trials: Baseline Adjustment},
  shorttitle = {Reduced Sample Sizes for Atrophy Outcomes in {{Alzheimer}}'s Disease Trials},
  author = {Schott, J. M. and Bartlett, J. W. and Barnes, J. and Leung, K. K. and Ourselin, S. and Fox, N. C.},
  year = {2010},
  month = aug,
  journal = {Neurobiology of Aging},
  series = {Alzheimer's {{Disease Neuroimaging Initiative}} ({{ADNI}}) {{Studies}}},
  volume = {31},
  number = {8},
  pages = {1452-1462.e2},
  issn = {0197-4580},
  doi = {10.1016/j.neurobiolaging.2010.04.011},
  urldate = {2022-10-20},
  abstract = {Cerebral atrophy rate is increasingly used as an outcome measure for Alzheimer's disease (AD) trials. We used the Alzheimer's disease Neuroimaging initiative (ADNI) dataset to assess if adjusting for baseline characteristics can reduce sample sizes. Controls (n = 199), patients with mild cognitive impairment (MCI) (n = 334) and AD (n = 144) had two MRI scans, 1-year apart; {$\sim$} 55\% had baseline CSF tau, p-tau, and A{$\beta$}1-42. Whole brain (KN\textendash BSI) and hippocampal (HMAPS-HBSI) atrophy rate, and ventricular expansion (VBSI) were calculated for each group; numbers required to power a placebo-controlled trial were estimated. Sample sizes per arm (80\% power, 25\% absolute rate reduction) for AD were (95\% CI): brain atrophy = 81 (64,109), hippocampal atrophy = 88 (68,119), ventricular expansion = 118 (92,157); and for MCI: brain atrophy = 149 (122,188), hippocampal atrophy = 201 (160,262), ventricular expansion = 234 (191,295). To detect a 25\% reduction relative to normal aging required increased sample sizes {$\sim$} 3-fold (AD), and {$\sim$} 5-fold (MCI). Disease severity and A{$\beta$}1-42 contributed significantly to atrophy rate variability. Adjusting for 11 predefined covariates reduced sample sizes by up to 30\%. Treatment trials in AD should consider the effects of normal aging; adjusting for baseline characteristics can significantly reduce required sample sizes.},
  langid = {english},
  keywords = {Alzheimer's disease,Biomarker,Clinical Trials,CSF,MRI},
  file = {/Users/zenn/Zotero/storage/NABF34FB/Schott et al. - 2010 - Reduced sample sizes for atrophy outcomes in Alzhe.pdf;/Users/zenn/Zotero/storage/QAY7RCBM/S0197458010001764.html}
}

@article{sperlingA4StudyStopping2014,
  title = {The {{A4 Study}}: {{Stopping AD Before Symptoms Begin}}?},
  shorttitle = {The {{A4 Study}}},
  author = {Sperling, Reisa A. and Rentz, Dorene M. and Johnson, Keith A. and Karlawish, Jason and Donohue, Michael and Salmon, David P. and Aisen, Paul},
  year = {2014},
  month = mar,
  journal = {Science Translational Medicine},
  volume = {6},
  number = {228},
  pages = {228fs13-228fs13},
  publisher = {{American Association for the Advancement of Science}},
  doi = {10.1126/scitranslmed.3007941},
  urldate = {2022-10-20},
  file = {/Users/zenn/Zotero/storage/BNRGWDKI/Sperling et al. - 2014 - The A4 Study Stopping AD Before Symptoms Begin.pdf}
}

@article{wangProportionalConstrainedLongitudinal2022,
  title = {Proportional Constrained Longitudinal Data Analysis Models for Clinical Trials in Sporadic {{Alzheimer}}'s Disease},
  author = {Wang, Guoqiao and Liu, Lei and Li, Yan and Aschenbrenner, Andrew J. and Bateman, Randall J. and Delmar, Paul and Schneider, Lon S. and Kennedy, Richard E. and Cutter, Gary R. and Xiong, Chengjie},
  year = {2022},
  month = jan,
  journal = {Alzheimer's \& Dementia: Translational Research \& Clinical Interventions},
  volume = {8},
  number = {1},
  issn = {2352-8737, 2352-8737},
  doi = {10.1002/trc2.12286},
  urldate = {2022-10-19},
  abstract = {Introduction: Clinical trials for sporadic Alzheimer's disease generally use mixed models for repeated measures (MMRM) or, to a lesser degree, constrained longitudinal data analysis models (cLDA) as the analysis model with time since baseline as a categorical variable. Inferences using MMRM/cLDA focus on the between-group contrast at the pre-determined, end-of-study assessments, thus are less efficient (eg, less power).},
  langid = {english},
  file = {/Users/zenn/Zotero/storage/MK38Z9MB/Wang et al. - 2022 - Proportional constrained longitudinal data analysi.pdf}
}

@article{wangTwoPeriodLinear2019,
  title = {Two-period Linear Mixed Effects Models to Analyze Clinical Trials with Run-in Data When the Primary Outcome Is Continuous: {{Applications}} to {{Alzheimer}}'s Disease},
  shorttitle = {Two-period Linear Mixed Effects Models to Analyze Clinical Trials with Run-in Data When the Primary Outcome Is Continuous},
  author = {Wang, Guoqiao and Aschenbrenner, Andrew J. and Li, Yan and McDade, Eric and Liu, Lei and Benzinger, Tammie L.S. and Bateman, Randall J. and Morris, John C. and Hassenstab, Jason J. and Xiong, Chengjie},
  year = {2019},
  month = jan,
  journal = {Alzheimer's \& Dementia: Translational Research \& Clinical Interventions},
  volume = {5},
  number = {1},
  pages = {450--457},
  issn = {2352-8737, 2352-8737},
  doi = {10.1016/j.trci.2019.07.007},
  urldate = {2023-04-14},
  langid = {english},
  file = {/Users/zenn/Zotero/storage/3SG3MES3/Wang et al. - 2019 - Two‐period linear mixed effects models to analyze .pdf}
}
